{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5191eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import glob\n",
    "from scipy.io import loadmat\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import interpolate\n",
    "import datetime\n",
    "import scipy \n",
    "import pickle\n",
    "import cartopy.crs as ccrs\n",
    "import scipy.spatial as spatial\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = ccrs.Stereographic(\n",
    "    central_latitude=90.0,\n",
    "    central_longitude=-45.0,\n",
    "    false_easting=0.0,\n",
    "    false_northing=0.0,\n",
    "    true_scale_latitude=70.0,\n",
    "    globe=ccrs.Globe('WGS84'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/scratch/icebridgedata/IRSNO1B_SnowRadar_Greenland_Repeat_Flights/20*/flat100/*mat')\n",
    "files2 = []\n",
    "for file in files:\n",
    "    if 'deconv' not in file:\n",
    "        files2.append(file)\n",
    "    else:\n",
    "        print('bad')\n",
    "        \n",
    "        \n",
    "fvals = np.asarray(files2)\n",
    "files_sorted = np.sort(fvals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8239f4a",
   "metadata": {},
   "source": [
    "## Sort files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc07c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "midlons=[]\n",
    "midlats=[]\n",
    "\n",
    "\n",
    "\n",
    "for file in files_sorted:\n",
    "    print(file)\n",
    "    ds = loadmat(file)\n",
    "    lat = ds['Latitude'][0]\n",
    "    midlat = lat[int(len(lat)/2)]\n",
    "    lon = ds['Longitude'][0]\n",
    "    midlon = lon[int(len(lon)/2)]\n",
    "    midlats.append(midlat)\n",
    "    midlons.append(midlon)\n",
    "\n",
    "\n",
    "pickle.dump({'Midlons':midlons,'midlats':midlats,'files':np.asarray(files_sorted)},open('Locations_of_Greenland_Box_Echograms_S.p','wb'))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5442ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = pickle.load(open('/data/users/mdattler/Large Files/Locations_of_Greenland_Box_Echograms_S.p','rb'))\n",
    "\n",
    "midlons = stuff['Midlons']\n",
    "midlats = stuff['midlats']\n",
    "files_sorted = stuff['files']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcefdc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a986e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zee_point = projection.transform_points(ccrs.PlateCarree(),np.asarray(midlons),np.asarray(midlats))\n",
    "\n",
    "xs = zee_point[:,0]\n",
    "ys = zee_point[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ed178",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = xs\n",
    "yvals = ys\n",
    "\n",
    "points = np.vstack((xvals,yvals)).T\n",
    "\n",
    "point_tree = spatial.cKDTree(points)\n",
    "\n",
    "catslen = []\n",
    "\n",
    "cats = []\n",
    "\n",
    "catsname = []\n",
    "catsnamelen = []\n",
    "fvals = np.asarray(files_sorted)\n",
    "nearby = []\n",
    "\n",
    "for num in np.arange(0,len(xvals)):\n",
    "    cat = point_tree.query_ball_point([xvals[num],yvals[num]],5200.)\n",
    "    \n",
    "    \n",
    "    nearby.append(fvals[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04591401",
   "metadata": {},
   "outputs": [],
   "source": [
    "allnear = []\n",
    "for nearie in nearby:\n",
    "    allnear.append(np.unique(nearie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearbys = []\n",
    "fvals = np.asarray(files_sorted)\n",
    "for ind,nearby_ind in enumerate(allnear):\n",
    "    nearby_its = []\n",
    "    for near in nearby_ind:\n",
    "        if fvals[ind][35+31:39+31] not in near:\n",
    "            nearby_its.append(near)\n",
    "    nearbys.append(nearby_its)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files = []\n",
    "filelen = []\n",
    "for ind, nearx in enumerate(nearbys):\n",
    "    my1file = [files_sorted[ind]]\n",
    "    \n",
    "    extra_files = nearx\n",
    "\n",
    "        \n",
    "        \n",
    "    filelen.append(len(extra_files))\n",
    "    \n",
    "    for extra_file in extra_files:\n",
    "        my1file.append(extra_file)\n",
    "    \n",
    "    total_files.append(my1file)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "sliced_files = total_files\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ceb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_finished_files = []\n",
    "for sliced_file in sliced_files:\n",
    "    if len(sliced_file)>1:\n",
    "        pre_finished_files.append(sliced_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cae390",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_finished_files2 = []\n",
    "\n",
    "for ind,file in enumerate(pre_finished_files):\n",
    "    first_file = pre_finished_files[ind][0]\n",
    "    first_year = int(first_file[35+31:39+31])\n",
    "    first_month = int(first_file[40+31:42+31])\n",
    "    first_day = int(first_file[43+31:45+31])\n",
    "    first_date = datetime.datetime(first_year,first_month,first_day)\n",
    "    \n",
    "    add_files = [first_file]\n",
    "    \n",
    "    for second_file in pre_finished_files[ind][1:]:\n",
    "        second_year = int(second_file[35+31:39+31])\n",
    "        second_month = int(second_file[40+31:42+31])\n",
    "        second_day = int(second_file[43+31:45+31])\n",
    "        second_date = datetime.datetime(second_year,second_month,second_day)\n",
    "        if first_date > second_date:\n",
    "            add_files.append(second_file)\n",
    "\n",
    "            \n",
    "    pre_finished_files2.append(add_files)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b4c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_files = []\n",
    "for sliced_file in pre_finished_files2:\n",
    "    if len(sliced_file)>1:\n",
    "        finished_files.append(sliced_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b7f39-5e3c-45d0-ab70-2ca2d3458260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def combining_files(finished_file):\n",
    "    plt.ioff()\n",
    "    first_file = finished_file[0]\n",
    "\n",
    "    path =  first_file[:76]+'/close_files'\n",
    "    pathext= path+'/'+first_file[-27:-4]\n",
    "    \n",
    "    first_ds = loadmat(first_file)\n",
    "    \n",
    "    first_lat = first_ds['Latitude'][0]\n",
    "    first_lon = first_ds['Longitude'][0]\n",
    "\n",
    "\n",
    "    \n",
    "    excess_files = np.asarray(finished_file[1:])\n",
    "\n",
    "    younger_files = np.asarray(excess_files)\n",
    "    \n",
    "    end_of_files = []\n",
    "    begin_of_files = []\n",
    "    for younger_file in younger_files:\n",
    "        end_of_files.append(int(younger_file[-7:-4]))\n",
    "        begin_of_files.append(younger_file[:-8])\n",
    "    \n",
    "    end_of_files=np.asarray(end_of_files)\n",
    "    begin_of_files=np.asarray(begin_of_files)\n",
    "    list_of_files = []\n",
    "    for dates in excess_files:\n",
    "        theslice =  np.abs(end_of_files - int(dates[-7:-4]))<2\n",
    "        list_of_files.append(list(excess_files[(begin_of_files == dates[:-8]) & (theslice)]))\n",
    "    \n",
    "\n",
    "\n",
    "    sets_of_files = set(frozenset(i) for i in list_of_files)\n",
    "    \n",
    "    list_of_files = list(sets_of_files)\n",
    "\n",
    "    new_list_of_files=[]\n",
    "    for index,item in enumerate(sets_of_files):\n",
    "        mysum=0\n",
    "        for itemz in sets_of_files:\n",
    "            if set(item).issubset(itemz):\n",
    "                if item != itemz:\n",
    "                    mysum+=1 \n",
    "        if mysum == 0:\n",
    "            new_list_of_files.append(item)\n",
    "    \n",
    "    lengths=[]\n",
    "\n",
    "    for filelist in new_list_of_files:\n",
    "        lengths.append(len(filelist))\n",
    "    maxlength = max(lengths)\n",
    "    list_of_files_padded = []\n",
    "    str_len = len(first_file)\n",
    "    for filelist in new_list_of_files:\n",
    "        the_add = maxlength-len(filelist)\n",
    "        tempfileset = list(filelist)\n",
    "        if the_add > 0:\n",
    "            for ind in np.arange(0,the_add):\n",
    "                tempfileset.append(\"0\"*str_len)\n",
    "        list_of_files_padded.append(tempfileset)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    shorted_list_of_files = np.unique(list_of_files_padded,axis=0).tolist() # ADD THIS BACK IN!!\n",
    "\n",
    "    if not isinstance(shorted_list_of_files[0], list):\n",
    "        print('ooooh isinstance coming into play')\n",
    "        shorted_list_of_files = [shorted_list_of_files]\n",
    "    for file_item in shorted_list_of_files:\n",
    "        temp_file_item = list(file_item)\n",
    "        if \"0\"*112 in temp_file_item:\n",
    "            temp_file_item.remove(\"0\"*str_len)\n",
    "\n",
    "        if len(temp_file_item) == 3:\n",
    "            doesitexist_2sub1 = os.path.exists(path+'/'+first_file[-27:-4]+'_'+temp_file_item[0][-27:-4]+'initial.mat')\n",
    "            doesitexist_2sub2 = os.path.exists(path+'/'+first_file[-27:-4]+'_'+temp_file_item[1][-27:-4]+'initial.mat')\n",
    "            doesitexist_2sub3 = os.path.exists(path+'/'+first_file[-27:-4]+'_'+temp_file_item[2][-27:-4]+'initial.mat')\n",
    "\n",
    "            if (doesitexist_2sub1==False) & (doesitexist_2sub2==False) & (doesitexist_2sub3==False):\n",
    "                continue         \n",
    "\n",
    "            flagging = np.asarray([doesitexist_2sub1,doesitexist_2sub2,doesitexist_2sub3])          \n",
    "            temp_file_item_temp=np.asarray(temp_file_item)[flagging==True]\n",
    "            temp_file_item = list(temp_file_item_temp)\n",
    "            print(temp_file_item)\n",
    "\n",
    "        if len(temp_file_item) == 1:\n",
    "\n",
    "            doesitexist_1 = os.path.exists(path+'/'+first_file[-27:-4]+'_'+temp_file_item[0][-27:-4]+'initial.mat')\n",
    "            if doesitexist_1 == False:\n",
    "                continue                \n",
    "        \n",
    "        if len(temp_file_item) == 2:\n",
    "            doesitexist_2sub1 = os.path.exists(path+'/'+first_file[-27:-4]+'_'+temp_file_item[0][-27:-4]+'initial.mat')\n",
    "            doesitexist_2sub2 = os.path.exists(path+'/'+first_file[-27:-4]+'_'+temp_file_item[1][-27:-4]+'initial.mat')\n",
    "\n",
    "            if (doesitexist_2sub1==False) & (doesitexist_2sub2==False):\n",
    "                continue         \n",
    "                \n",
    "            flagging = np.asarray([doesitexist_2sub1,doesitexist_2sub2])     \n",
    "                \n",
    "\n",
    "            temp_file_item_temp=np.asarray(temp_file_item)[flagging==True]\n",
    "\n",
    "            flagging=np.asarray(flagging)\n",
    "            temp_file_item_temp=np.asarray(temp_file_item)[flagging==1]\n",
    "            temp_file_item = list(temp_file_item_temp)\n",
    "            \n",
    "        if len(temp_file_item) == 3:\n",
    "            print('ayoo, there are three files')\n",
    "            doesitexist_2sub1 = os.path.exists(path+'/'+first_file[-27:-4]+'_'+temp_file_item[0][-27:-4]+'initial.mat')\n",
    "            doesitexist_2sub2 = os.path.exists(path+'/'+first_file[-27:-4]+'_'+temp_file_item[1][-27:-4]+'initial.mat')\n",
    "            doesitexist_2sub3 = os.path.exists(path+'/'+first_file[-27:-4]+'_'+temp_file_item[2][-27:-4]+'initial.mat')\n",
    "\n",
    "            if (doesitexist_2sub1==False) & (doesitexist_2sub2==False) & (doesitexist_2sub3==False):\n",
    "                continue         \n",
    "                \n",
    "            flagging = np.asarray([doesitexist_2sub1,doesitexist_2sub2,doesitexist_2sub3])          \n",
    "\n",
    "            temp_file_item_temp=np.asarray(temp_file_item)[flagging==True]\n",
    "\n",
    "            flagging=np.asarray(flagging)\n",
    "            temp_file_item_temp=np.asarray(temp_file_item)[flagging==1]\n",
    "            temp_file_item = list(temp_file_item_temp)\n",
    "\n",
    "            if len(temp_file_item) > 2:\n",
    "                temp_file_item = np.sort(temp_file_item)[:2]\n",
    "                \n",
    "        \n",
    "            \n",
    "        if len(temp_file_item) == 1:\n",
    "            mydata = scipy.io.loadmat(path+'/'+first_file[-27:-4]+'_'+temp_file_item[0][-27:-4]+'initial.mat')\n",
    "            filename = path+'/'+first_file[-27:-4]+'_'+temp_file_item[0][-27:-4]+'_add_none_new_FINAL.mat'\n",
    "\n",
    "            if np.sum(~np.isnan(mydata['Secondary_Data'])) == 0:\n",
    "                print('One file, and not enough data')\n",
    "                continue\n",
    "            scipy.io.savemat(filename,{'Primary_Data':mydata['Primary_Data'],\\\n",
    "                    'Primary_Lat':mydata['Primary_Lat'],'Primary_Lon':mydata['Primary_Lon'],\\\n",
    "                    'Secondary_Data':mydata['Secondary_Data'],\\\n",
    "                    'Mean_200_primary':mydata['Mean_200_primary'],'Mean_200_sec':mydata['Mean_200_sec'],\\\n",
    "                    'StDev_200_primary':mydata['StDev_200_primary'],'StDev_200_sec':mydata['StDev_200_sec'],\\\n",
    "                    'del_t2':mydata['del_t2'],'del_t1':mydata['del_t1']})   \n",
    "\n",
    "            fig_extra, (ax_extra1,ax_extra2) = plt.subplots(nrows=2, ncols=1, figsize=(6,18))\n",
    "            \n",
    "            ax_extra1.set_title(first_file[-27:-4])\n",
    "            ax_extra2.set_title(temp_file_item[0][-27:-4])\n",
    "\n",
    "            [m,n]=np.meshgrid(np.arange(0,mydata['Primary_Data'].shape[1])*0.1,np.arange(0,mydata['Primary_Data'].shape[0]))\n",
    "            ax_extra1.pcolor(m[::5,:],n[::5,:],mydata['Primary_Data'][::5,:],cmap='Blues')\n",
    "            ax_extra2.pcolor(m[::5,:],n[::5,:],mydata['Secondary_Data'][::5,:],cmap='Blues')\n",
    "            ax_extra1.invert_yaxis()\n",
    "            ax_extra2.invert_yaxis()\n",
    "            fig_extra.savefig(filename+'.png')\n",
    "            print(filename+'.png')\n",
    "            plt.close()\n",
    "            del mydata\n",
    "          \n",
    "            \n",
    "        if len(temp_file_item) == 2:\n",
    "            mydata = scipy.io.loadmat(path+'/'+first_file[-27:-4]+'_'+temp_file_item[0][-27:-4]+'initial.mat')    \n",
    "            mydata2 = scipy.io.loadmat(path+'/'+first_file[-27:-4]+'_'+temp_file_item[1][-27:-4]+'initial.mat')\n",
    "            first_sec_data = mydata['Secondary_Data']\n",
    "            first_sec_data[~np.isnan(mydata2['Secondary_Data'])] = np.nan\n",
    "            sec_sec_data = mydata2['Secondary_Data']    \n",
    "            total_sec_data=np.nansum(np.dstack((first_sec_data,sec_sec_data)),2)\n",
    "\n",
    "            nanidx = np.isnan(first_sec_data)*np.isnan(sec_sec_data)\n",
    "\n",
    "            \n",
    "            total_sec_data[nanidx] = np.nan\n",
    "            \n",
    "\n",
    "            if np.sum(~np.isnan(total_sec_data)) == 0:\n",
    "                print('Flights too far apart, no data here')\n",
    "                continue\n",
    "            \n",
    "            filename = path+'/'+first_file[-27:-4]+'_'+temp_file_item[0][-27:-4]+'_add_'+temp_file_item[1][-27:-4]+'_FINAL.mat'\n",
    "            scipy.io.savemat(filename,{'Primary_Data':mydata['Primary_Data'],\\\n",
    "                        'Primary_Lat':mydata['Primary_Lat'],'Primary_Lon':mydata['Primary_Lon'],\\\n",
    "                        'Secondary_Data':total_sec_data,\\\n",
    "                        'Mean_200_primary':(mydata['Mean_200_primary']+mydata2['Mean_200_primary'])/2,'Mean_200_sec':(mydata['Mean_200_sec']+mydata2['Mean_200_sec'])/2,\\\n",
    "                        'StDev_200_primary':(mydata['StDev_200_primary']+mydata2['StDev_200_primary'])/2,'StDev_200_sec':(mydata['StDev_200_sec']+mydata2['StDev_200_sec'])/2,\\\n",
    "                        'del_t2':mydata['del_t2'],'del_t1':mydata['del_t1']})     \n",
    "            fig_extra, (ax_extra1,ax_extra2) = plt.subplots(nrows=2, ncols=1, figsize=(6,18))\n",
    "            \n",
    "            ax_extra1.set_title(first_file[-27:-4])\n",
    "            ax_extra2.set_title(temp_file_item[0][-27:-4]+', '+temp_file_item[1][-27:-4])\n",
    "\n",
    "            [m,n]=np.meshgrid(np.arange(0,mydata['Primary_Data'].shape[1])*0.1,np.arange(0,mydata['Primary_Data'].shape[0]))\n",
    "            cax2=ax_extra1.pcolor(m[::5,:],n[::5,:],mydata['Primary_Data'][::5,:],cmap='Blues')\n",
    "            cax=ax_extra2.pcolor(m[::5,:],n[::5,:],total_sec_data[::5,:],cmap='Blues')\n",
    "            plt.colorbar(cax2,ax=ax_extra1)\n",
    "            plt.colorbar(cax,ax=ax_extra2)\n",
    "            ax_extra1.invert_yaxis()\n",
    "            ax_extra2.invert_yaxis()\n",
    "            fig_extra.savefig(filename+'.png')\n",
    "            print(filename+'.png')\n",
    "            plt.close()\n",
    "\n",
    "            if len(temp_file_item)>2:\n",
    "                print(temp_file_item)\n",
    "                print(len(temp_file_item))\n",
    "                print('OHHHH NOOO!!')\n",
    "                \n",
    "            del mydata, mydata2\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8da862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lining_up_lines(finished_file):\n",
    "    plt.ioff()\n",
    "    first_file = finished_file[0]\n",
    "    \n",
    "    # first_year = first_file[37:41]\n",
    "    # first_month = first_file[41:43]\n",
    "    # first_day = first_file[43:45]\n",
    "    \n",
    "    # first_date = datetime.datetime(int(first_year),int(first_month),int(first_day))\n",
    "    path =  first_file[:76]+'/close_files'\n",
    "    pathext= path+'/'+first_file[-27:-4]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # isExist = os.path.isfile(first_file[:46]+'close_files_pic/'+first_file[-27:-4]+'_NearbyFiles_with200_3_new.png')\n",
    "    # if isExist==True:\n",
    "    #     return'\n",
    "    second_file_temp=finished_file[-1]\n",
    "    isExist2 = os.path.isfile(path+'/'+first_file[-27:-4]+'_'+second_file_temp[-27:-4]+'matching_file_FINAL.png')\n",
    "    # if isExist2==True:\n",
    "    #     return\n",
    "    \n",
    "    first_ds = loadmat(first_file)\n",
    "    \n",
    "    first_lat = first_ds['Latitude'][0]\n",
    "    first_lon = first_ds['Longitude'][0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    first_time_orig = np.squeeze(np.arange(0,len(first_ds['Data'][200:,0]))*first_ds['del_t']*10**-6)\n",
    "    \n",
    "    \n",
    "    first_time = np.squeeze(np.arange(0,len(first_ds['Data'][200:,0]))*first_ds['del_t']*10**-6)\n",
    "    first_data = first_ds['Data'][200:,:]\n",
    "\n",
    "    \n",
    "    next_file_ind = 1\n",
    "    second_dats = []\n",
    "    second_ts = []\n",
    "    second_ds = []\n",
    "    \n",
    "    for next_file_ind,second_file in enumerate(finished_file[1:]):\n",
    "        path =  first_file[:46+34-4]+'/close_files'\n",
    "        second_ds_mat = loadmat(second_file[:-4])\n",
    "        second_lat = second_ds_mat['Latitude'][0]\n",
    "        second_lon = second_ds_mat['Longitude'][0]\n",
    "        second_time = np.squeeze(np.arange(0,len(second_ds_mat['Data'][200:,0]))*second_ds_mat['del_t']*10**-6)\n",
    "    \n",
    "    \n",
    "        second_data = np.zeros((len(second_ds_mat['Data'][0,:]),len(first_time))).T*np.nan\n",
    "    \n",
    "        for along_data_ind in np.arange(0, second_data.shape[1]):\n",
    "            y = second_ds_mat['Data'][200:,along_data_ind]\n",
    "            f = interpolate.interp1d(np.squeeze(second_time),y,fill_value=\"extrapolate\")\n",
    "            second_data[:,along_data_ind]=f(first_time)\n",
    "        \n",
    "        maxarraysize=np.max([first_data.shape[0],second_data.shape[0]])\n",
    "        d,t = np.meshgrid(np.arange(0,first_data.shape[1])/10.,first_time)\n",
    "        d2,t2 = np.meshgrid(np.arange(0,first_data.shape[1])/10.,first_time)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        allocate_first = np.zeros((maxarraysize,first_data.shape[1])).T*np.nan\n",
    "        allocate_second = np.zeros((maxarraysize,second_data.shape[1])).T*np.nan\n",
    "    \n",
    "        allocate_first[:first_data.shape[1],:first_data.shape[0]] = first_data.T.copy()\n",
    "        \n",
    "        allocate_second[:second_data.shape[1],:second_data.shape[0]] = second_data.T.copy()\n",
    "    \n",
    "    \n",
    "              \n",
    "    \n",
    "    \n",
    "        alldata = np.hstack((allocate_first.T,allocate_second.T)).T\n",
    "        \n",
    "        mean200_pri = np.nanmean(first_ds['Data'][:200,:])\n",
    "        mean200_sec = np.nanmean(second_ds_mat['Data'][:200,:])\n",
    "    \n",
    "        stdev200_pri = np.mean(np.nanstd(first_ds['Data'][:200,:],1))\n",
    "        stdev200_sec = np.mean(np.nanstd(second_ds_mat['Data'][:200,:],1))\n",
    "    \n",
    "        # alldata = np.hstack((first_data,second_data))\n",
    "    \n",
    "        new_col = alldata.sum(1)[...,None] # None keeps (n, 1) shape\n",
    "        new_col[:,0] = np.nan\n",
    "        alldata = np.append(alldata, new_col, 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "        zee_point = projection.transform_points(ccrs.PlateCarree(),np.asarray(first_lon),np.asarray(first_lat))\n",
    "        zee_point_2 = projection.transform_points(ccrs.PlateCarree(),np.asarray(second_lon),np.asarray(second_lat))\n",
    "    \n",
    "        xvals_first = zee_point[:,0]\n",
    "        yvals_first = zee_point[:,1]\n",
    "        xvals_second = zee_point_2[:,0]\n",
    "        yvals_second = zee_point_2[:,1]\n",
    "        spacing = np.sqrt((xvals_first[2]-xvals_first[1])**2 + (yvals_first[2]-yvals_first[1])**2)\n",
    "\n",
    "        points = np.vstack((xvals_second,yvals_second)).T\n",
    "    \n",
    "        point_tree2 = spatial.KDTree(points)\n",
    "    \n",
    "        cats = np.zeros((len(xvals_first),20))-1\n",
    "        distances=[]\n",
    "        for num in np.arange(0,len(xvals_first)):\n",
    "            cat2 = point_tree2.query([xvals_first[num],yvals_first[num]],k=50,)\n",
    "            cat = cat2[1]\n",
    "            max_distance = 100\n",
    "            cat[cat2[0]>max_distance] = -1\n",
    "            distances.append(cat2[0][0])\n",
    "            cats[num] = int(cat[0]) \n",
    "    \n",
    "    \n",
    "            \n",
    "                \n",
    "    \n",
    "        del cat2, point_tree2\n",
    "    \n",
    "    \n",
    "        if next_file_ind == 0:\n",
    "            first_t = t[200:,:len(first_lat)]-t[200,:len(first_lat)]\n",
    "            first_d = d[200:,:len(first_lat)]-d[200,:len(first_lat)]\n",
    "            indices = cats.astype(int)\n",
    "            zee_point1 = projection.transform_points(ccrs.PlateCarree(),np.asarray(first_lon),np.asarray(first_lat))\n",
    "\n",
    "        \n",
    "    \n",
    "        zee_point2 = projection.transform_points(ccrs.PlateCarree(),np.asarray(second_lon),np.asarray(second_lat))\n",
    "        \n",
    "        indices = cats[:,1].astype(int)\n",
    "    \n",
    "        new_col = allocate_second.T.sum(1)[...,None] # None keeps (n, 1) shape\n",
    "        new_col[:,0] = np.nan\n",
    "        allocate_second_filled = np.append(allocate_second.T, new_col, 1).T\n",
    "        \n",
    "    \n",
    "        new_data = np.zeros(allocate_first.shape)\n",
    "    \n",
    "        for ind,indice in enumerate(indices):\n",
    "            new_data[ind,:] = allocate_second_filled[indice,:]\n",
    "            \n",
    "            if indice == -1:\n",
    "                continue\n",
    "    \n",
    "        second_dat = new_data\n",
    "    \n",
    "    \n",
    "        if np.sum(~np.isnan(new_data))==0:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        fig_extra, (ax_extra1,ax_extra2) = plt.subplots(nrows=2, ncols=1, figsize=(6,18))\n",
    "        \n",
    "        ax_extra1.set_title(first_file[-27:-4])\n",
    "        ax_extra2.set_title(second_file[-27:-4])\n",
    "        indices = cats.astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        first_dat = first_data\n",
    "    \n",
    "        ax_extra1.pcolormesh(d,t,allocate_first.T,cmap='Blues',shading='nearest',)\n",
    "        first_t = t[:,:len(first_lat)]\n",
    "    \n",
    "        ax_extra1.set_ylim(0.2*10**-6,0)\n",
    "        ax_extra2.set_ylim(0.2*10**-6,0)\n",
    "        ax_extra2.set_xlabel('Distance [km]')\n",
    "        ax_extra2.set_ylabel('twtt [s]')\n",
    "        ax_extra2.pcolormesh(d2,t2,new_data.T,cmap='Blues')\n",
    "        \n",
    "        isExist2 = os.path.exists(path)\n",
    "        \n",
    "        if isExist2 == False:\n",
    "            os.makedirs(path)\n",
    "            \n",
    "        fig_extra.savefig(path+'/'+first_file[-27:-4]+'_'+second_file[-27:-4]+'_initial.png')\n",
    "    \n",
    "    \n",
    "        \n",
    "        scipy.io.savemat(path+'/'+first_file[-27:-4]+'_'+second_file[-27:-4]+'initial.mat',{'Primary_Data':allocate_first.T,\\\n",
    "                                                                        'Primary_Depths':first_d,\\\n",
    "                                                                        'Primary_Lat':first_lat,'Primary_Lon':first_lon,\\\n",
    "                                                                        'Secondary_Data':new_data.T,\\\n",
    "                                                                        'Mean_200_primary':mean200_pri,'Mean_200_sec':mean200_sec,\\\n",
    "                                                                        'StDev_200_primary':stdev200_pri,'StDev_200_sec':stdev200_sec,\\\n",
    "                                                                        'del_t2':first_ds['del_t'],'del_t1':first_ds['del_t'],'distances':distances})    \n",
    "       \n",
    "    \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        isExist = os.path.exists(path)\n",
    "    \n",
    "        if isExist == False:\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    del first_t, first_d, second_dat, second_data, first_data, cats, distances, alldata, first_ds, second_ds_mat, allocate_first, allocate_second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e79ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind,finished_file in enumerate(finished_files):\n",
    "    \n",
    "    if '2018.04.23' not in finished_file[0]:\n",
    "        continue\n",
    "    plt.ioff()\n",
    "    lining_up_lines(finished_file)\n",
    "    combining_files(finished_file)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popcorn",
   "language": "python",
   "name": "popcorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
